{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Emotion Classification Model Comparison\n",
                "\n",
                "This notebook compares the performance of three popular transformer models on the Emotion dataset:\n",
                "1. DistilBERT (`distilbert-base-uncased`)\n",
                "2. BERT (`bert-base-uncased`)\n",
                "3. RoBERTa (`roberta-base`)\n",
                "\n",
                "We will fine-tune each model and evaluate their Accuracy and F1 Score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
                "from sklearn.metrics import accuracy_score, f1_score\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "\n",
                "# Set device\n",
                "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Dataset\n",
                "We use the `dair-ai/emotion` dataset from Hugging Face."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "emotions = load_dataset(\"dair-ai/emotion\")\n",
                "print(emotions)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Define Metrics and Training Loop\n",
                "We define a helper function to compute metrics (Accuracy & F1) and a main function to tokenize, train, and evaluate a given model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(pred):\n",
                "    labels = pred.label_ids\n",
                "    preds = pred.predictions.argmax(-1)\n",
                "    acc = accuracy_score(labels, preds)\n",
                "    f1 = f1_score(labels, preds, average='weighted')\n",
                "    return {'accuracy': acc, 'f1': f1}\n",
                "\n",
                "def train_and_evaluate(model_ckpt):\n",
                "    print(f\"\\n{'='*30}\")\n",
                "    print(f\"Processing model: {model_ckpt}\")\n",
                "    print(f\"{'='*30}\")\n",
                "    \n",
                "    # 1. Tokenizer\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
                "    \n",
                "    def tokenize(batch):\n",
                "        return tokenizer(batch['text'], padding=True, truncation=True)\n",
                "    \n",
                "    print(f\"Tokenizing data for {model_ckpt}...\")\n",
                "    emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
                "    \n",
                "    # 2. Model Initialization\n",
                "    num_labels = 6\n",
                "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)\n",
                "    \n",
                "    # 3. Training Arguments\n",
                "    model_output_dir = f\"{model_ckpt}-emotion-finetuned\"\n",
                "    training_args = TrainingArguments(\n",
                "        output_dir=model_output_dir,\n",
                "        num_train_epochs=2,\n",
                "        learning_rate=2e-5,\n",
                "        per_device_train_batch_size=64,\n",
                "        per_device_eval_batch_size=64,\n",
                "        weight_decay=0.01,\n",
                "        eval_strategy=\"epoch\",\n",
                "        disable_tqdm=False,\n",
                "        logging_dir=f'{model_output_dir}/logs',\n",
                "        save_strategy=\"epoch\",\n",
                "        load_best_model_at_end=True,\n",
                "    )\n",
                "    \n",
                "    # 4. Trainer\n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        compute_metrics=compute_metrics,\n",
                "        train_dataset=emotions_encoded['train'],\n",
                "        eval_dataset=emotions_encoded['validation'],\n",
                "        processing_class=tokenizer\n",
                "    )\n",
                "    \n",
                "    # 5. Train\n",
                "    print(f\"Training {model_ckpt}...\")\n",
                "    trainer.train()\n",
                "    \n",
                "    # 6. Evaluate on Test Set\n",
                "    print(f\"Evaluating {model_ckpt} on test set...\")\n",
                "    eval_result = trainer.evaluate(emotions_encoded['test'])\n",
                "    \n",
                "    return {\n",
                "        \"Model\": model_ckpt,\n",
                "        \"Accuracy\": eval_result['eval_accuracy'],\n",
                "        \"F1 Score\": eval_result['eval_f1'],\n",
                "        \"Loss\": eval_result['eval_loss']\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Run Comparison\n",
                "We iterate through the list of models, training and evaluating each one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = [\n",
                "    \"distilbert-base-uncased\",\n",
                "    \"bert-base-uncased\",\n",
                "    \"roberta-base\"\n",
                "]\n",
                "\n",
                "results = []\n",
                "\n",
                "for model_name in models:\n",
                "    res = train_and_evaluate(model_name)\n",
                "    results.append(res)\n",
                "    \n",
                "    # Checkpoint results\n",
                "    df_temp = pd.DataFrame(results)\n",
                "    print(f\"\\nPartial Results:\\n{df_temp}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Final Results Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_final = pd.DataFrame(results)\n",
                "print(\"Final Comparison:\")\n",
                "print(df_final)\n",
                "\n",
                "# Plotting\n",
                "df_final.plot(x=\"Model\", y=[\"Accuracy\", \"F1 Score\"], kind=\"bar\", figsize=(10, 6), rot=0)\n",
                "plt.title(\"Model Comparison on Emotion Classification\")\n",
                "plt.ylabel(\"Score\")\n",
                "plt.ylim(0.8, 1.0)  # Adjust ylim based on expected performance\n",
                "plt.legend(loc='lower right')\n",
                "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}